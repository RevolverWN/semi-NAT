import argparse
from copy import deepcopy
from itertools import chain
from collections import abc as container_abcs
from collections import defaultdict

import torch
from torch.optim import Adam
from optimizers import register_optimizer
import torch.distributed as dist

default_dict = {
                "lr": {"type": float, "default": 5e-4, "help": "learning rate"},
                "betas": {"type": float, "nargs": 2, "default": (0.9, 0.98), "help": "betas for Adam optimizer"},
                "eps": {"type": float, "default": 1e-8, "help": "epsilon for Adam optimizer"},
                "weight_decay": {"type": float, "default": 0., "help": "weight decay"}
                }


@register_optimizer("adam")
class Seq2seqAdam(Adam):
    config = default_dict

    def __init__(self, model_params, config):
        self.config = config
        super(Seq2seqAdam, self).__init__(model_params,
                                          lr=self.config.lr,
                                          betas=self.config.betas,
                                          eps=self.config.eps,
                                          weight_decay=self.config.weight_decay)

    @classmethod
    def add_args(cls, parser: argparse.ArgumentParser):
        for param_name, param_attr in default_dict.items():
            parser.add_argument("--" + param_name, **param_attr)

    def state_dict(self):
        r"""Returns the state of the optimizer as a :class:`dict`.

        It contains two entries:

        * state - a dict holding current optimization state. Its content
            differs between optimizer classes.
        * param_groups - a dict containing all parameter groups
        """
        # Save order indices instead of Tensors
        param_mappings = {}
        start_index = 0

        def pack_group(group):
            nonlocal start_index
            packed = {k: v for k, v in group.items() if k != 'params'}
            param_mappings.update({id(p): i for i, p in enumerate(group['params'], start_index)
                                   if id(p) not in param_mappings})
            packed['params'] = [param_mappings[id(p)] for p in group['params']]
            start_index += len(packed['params'])
            return packed
        param_groups = [pack_group(g) for g in self.param_groups]
        # Remap state to use order indices as keys
        packed_state = {(param_mappings[id(k)] if isinstance(k, torch.Tensor) else k): v
                        for k, v in self.state.items()}
        return {
            'state': packed_state,
            'param_groups': param_groups,
        }

    def load_state_dict(self, state_dict):
        r"""Loads the optimizer state.

        Args:
            state_dict (dict): optimizer state. Should be an object returned
                from a call to :meth:`state_dict`.
        """
        # deepcopy, to be consistent with module API
        state_dict = deepcopy(state_dict)
        # Validate the state_dict
        dst_groups = self.param_groups
        saved_groups = state_dict['param_groups']

        if len(dst_groups) != len(saved_groups):
            raise ValueError("loaded state dict has a different number of "
                             "parameter groups")
        param_lens = (len(g['params']) for g in dst_groups)
        saved_lens = (len(g['params']) for g in saved_groups)
        if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):
            raise ValueError("loaded state dict contains a parameter group "
                             "that doesn't match the size of optimizer's group")

        # Update the state
        id_dst_map = {old_id: p for old_id, p in
                  zip(chain.from_iterable((g['params'] for g in saved_groups)),
                      chain.from_iterable((g['params'] for g in dst_groups)))}

        def cast(param, value):
            r"""Make a deep copy of value, casting all tensors to device of param."""
            if isinstance(value, torch.Tensor):
                # Floating-point types are a bit special here. They are the only ones
                # that are assumed to always match the type of params.
                if param.is_floating_point():
                    value = value.to(param.dtype)
                value = value.to(param.device)
                return value
            elif isinstance(value, dict):
                return {k: cast(param, v) for k, v in value.items()}
            elif isinstance(value, container_abcs.Iterable):
                return type(value)(cast(param, v) for v in value)
            else:
                return value

        # Copy state assigned to params (and cast tensors to appropriate types).
        # State that is not assigned to params is copied as is (needed for
        # backward compatibility).
        state = defaultdict(dict)
        for params_id, saved_params in state_dict['state'].items():
            if params_id in id_dst_map:
                dstparam = id_dst_map[params_id]
                state[dstparam] = cast(dstparam, saved_params)
            else:
                state[params_id] = saved_params

        # Update parameter groups, setting their 'params' value
        def update_group(group, new_group):
            new_group['params'] = group['params']
            return new_group

        param_groups = [
            update_group(g, ng) for g, ng in zip(dst_groups, saved_groups)]
        self.__setstate__({'state': state, 'param_groups': param_groups})
